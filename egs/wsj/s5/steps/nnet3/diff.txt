90,92c90,92
< if [ $# != 3 ]; then
<   echo "Usage: $0 [opts] <data> <spk-ali-dir> <exp-dir>"
<   echo " e.g.: $0 data/train exp/tri3_ali exp/tri4_nnet"
---
> if [ $# != 1 ]; then
>   echo "Usage: $0 [opts] <exp-dir>"
>   echo " e.g.: $0 exp/tri4_nnet"
134,136c134
< data=$1
< alidir_spk=$2
< dir=$3
---
> dir=$1
143,153c141,143
< # Check some files.
< for f in $data/feats.scp $alidir_spk/ali.ark $alidir_spk/spk_counts $alidir_spk/spk_num; do
<   [ ! -f $f ] && echo "$0: no such file $f" && exit 1;
< done
< 
< 
< # Set some variables.
< spk_num=`cat $alidir_spk/spk_num` || exit 1;
< num_leaves_spk=$spk_num
< [ -z $num_leaves_spk ] && echo "\$num_leaves_spk is unset" && exit 1
< [ "$num_leaves_spk" -eq "0" ] && echo "\$num_leaves_spk is 0" && exit 1
---
> # config file set by hand, num_hidden_layers in vars > 0 will be ok
> for f in $dir/configs/all.config $dir/configs/vars; do
>   [ ! -f $f ] && echo "$0: no such file $f, config file set by hand, from baseline " && exit 1;
156,158d145
< # in this dir we'll have just one job.
< sdata=$data/split$nj
< utils/split_data.sh $data $nj
163,205d149
< 
< # First work out the feature and iVector dimension, needed for tdnn config creation.
< case $feat_type in
<   raw) feat_dim=$(feat-to-dim --print-args=false scp:$data/feats.scp -) || \
<       { echo "$0: Error getting feature dim"; exit 1; }
<     ;;
<   *)
<    echo "$0: Bad --feat-type '$feat_type';"; exit 1;
< esac
< if [ -z "$online_ivector_dir" ]; then
<   ivector_dim=0
< else
<   ivector_dim=$(feat-to-dim scp:$online_ivector_dir/ivector_online.scp -) || exit 1;
< fi
< 
< 
< if [ $stage -le -5 ]; then
<   echo "$0: creating neural net configs";
< 
<   if [ ! -z "$relu_dim" ]; then
<     dim_opts="--relu-dim $relu_dim"
<   else
<     dim_opts="--pnorm-input-dim $pnorm_input_dim --pnorm-output-dim  $pnorm_output_dim"
<   fi
< 
<   # create the config files for nnet initialization for spk recognition
<   python steps/nnet3/make_tdnn_configs.py  \
<     --splice-indexes "$splice_indexes"  \
<     --feat-dim $feat_dim \
<     --ivector-dim $ivector_dim  \
<      $dim_opts \
<     --use-presoftmax-prior-scale $use_presoftmax_prior_scale \
<     --num-targets  $num_leaves_spk  \
<    $dir/configs || exit 1;
< 
<   # Initialize as "raw" nnet, prior to training the LDA-like preconditioning
<   # matrix.  This first config just does any initial splicing that we do;
<   # we do this as it's a convenient way to get the stats for the 'lda-like'
<   # transform.
<   $cmd $dir/log/nnet_init.log \
<     nnet3-init --srand=-2 $dir/configs/init.config $dir/init.raw || exit 1;
< fi
< 
218,230c162
<   extra_opts=()
<   [ ! -z "$cmvn_opts" ] && extra_opts+=(--cmvn-opts "$cmvn_opts")
<   [ ! -z "$feat_type" ] && extra_opts+=(--feat-type $feat_type)
<   [ ! -z "$online_ivector_dir" ] && extra_opts+=(--online-ivector-dir $online_ivector_dir)
<   extra_opts+=(--left-context $left_context)
<   extra_opts+=(--right-context $right_context)
<   echo "$0: calling get_egs_spk.sh "
<   steps/nnet3/get_egs_spk_pure.sh $egs_opts "${extra_opts[@]}" \
<       --samples-per-iter $samples_per_iter --stage $get_egs_stage \
<       --cmd "$cmd" $egs_opts \
<       --frames-per-eg $frames_per_eg --spk-num $spk_num \
<       $data $alidir_spk $dir/egs || exit 1;
< 
---
>   echo "egs merged by hand using merge_egs.sh " && exit 1
235,243d166
< if [ "$feat_dim" != "$(cat $egs_dir/info/feat_dim)" ]; then
<   echo "$0: feature dimension mismatch with egs, $feat_dim vs $(cat $egs_dir/info/feat_dim)";
<   exit 1;
< fi
< if [ "$ivector_dim" != "$(cat $egs_dir/info/ivector_dim)" ]; then
<   echo "$0: ivector dimension mismatch with egs, $ivector_dim vs $(cat $egs_dir/info/ivector_dim)";
<   exit 1;
< fi
< 
270,291c193
<   echo "$0: getting preconditioning matrix for input features."
<   num_lda_jobs=$num_archives
<   [ $num_lda_jobs -gt $max_lda_jobs ] && num_lda_jobs=$max_lda_jobs
< 
<   # Write stats with the same format as stats for LDA.
<   $cmd JOB=1:$num_lda_jobs $dir/log/get_lda_stats.JOB.log \
<       nnet3-acc-lda-stats --rand-prune=$rand_prune \
<         $dir/init.raw "ark:$egs_dir/egs.JOB.ark" $dir/JOB.lda_stats || exit 1;
< 
<   all_lda_accs=$(for n in $(seq $num_lda_jobs); do echo $dir/$n.lda_stats; done)
<   $cmd $dir/log/sum_transform_stats.log \
<     sum-lda-accs $dir/lda_stats $all_lda_accs || exit 1;
< 
<   rm $all_lda_accs || exit 1;
< 
<   # this computes a fixed affine transform computed in the way we described in
<   # Appendix C.6 of http://arxiv.org/pdf/1410.7455v6.pdf; it's a scaled variant
<   # of an LDA transform but without dimensionality reduction.
<   $cmd $dir/log/get_transform.log \
<      nnet-get-feature-transform $lda_opts $dir/lda.mat $dir/lda_stats || exit 1;
< 
<   ln -sf ../lda.mat $dir/configs/lda.mat
---
>   echo "$0: in config lda(_spk).vec directed to baseline which has the same corresponding model"
296,304c198
<   echo "$0: spk recognition, preparing initial vector for FixedScaleComponent before softmax"
<   echo "  ... using priors^$presoftmax_prior_scale_power and rescaling to average 1"
<   awk -v power=$presoftmax_prior_scale_power -v smooth=0.01 \
<      '{ for(i=2; i<=NF-1; i++) { count[i-2] = $i;  total += $i; }
<         num_pdfs=NF-2;  average_count = total/num_pdfs;
<         for (i=0; i<num_pdfs; i++) stot += (scale[i] = (count[i] + smooth * average_count)^power)
<         printf " [ "; for (i=0; i<num_pdfs; i++) printf("%f ", scale[i]*num_pdfs/stot); print "]" }' \
<      $alidir_spk/spk_counts > $dir/presoftmax_prior_scale.vec
<   ln -sf ../presoftmax_prior_scale.vec $dir/configs/presoftmax_prior_scale.vec
---
>   echo "$0: in config presoftmax_prior_scale(_spk).vec directed to baseline which has the same corresponding model"
308,311c202,204
<   # Add the first layer; this will add in the lda.mat and
<   # presoftmax_prior_scale.vec.
<   $cmd $dir/log/add_first_layer.log \
<        nnet3-init --srand=-3 $dir/init.raw $dir/configs/layer1.config $dir/0.raw || exit 1;
---
>   # Init the whole nnet
>   $cmd $dir/log/add_all_layers.log \
>     nnet3-init --srand=-3 $dir/configs/all.config $dir/0.raw || exit 1;
418,430c311,313
<     if [ $x -gt 0 ] && \
<       [ $x -le $[($num_hidden_layers-1)*$add_layers_period] ] && \
<       [ $[$x%$add_layers_period] -eq 0 ]; then
<       do_average=false # if we've just mixed up, don't do averaging but take the
<                        # best.
<       cur_num_hidden_layers=$[1+$x/$add_layers_period]
<       config=$dir/configs/layer$cur_num_hidden_layers.config
<       raw="nnet3-copy --learning-rate=$this_learning_rate $dir/$x.raw - | nnet3-init --srand=$x - $config - |"
<     else
<       do_average=true
<       if [ $x -eq 0 ]; then do_average=false; fi # on iteration 0, pick the best, don't average.
<       raw="nnet3-copy --learning-rate=$this_learning_rate $dir/$x.raw - |"
<     fi
---
>     do_average=true
>     if [ $x -eq 0 ]; then do_average=false; fi # on iteration 0, pick the best, don't average.
>     raw="nnet3-copy --learning-rate=$this_learning_rate $dir/$x.raw - |"
