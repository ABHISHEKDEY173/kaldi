2a3,4
> # note, TDNN is the same as what we used to call multisplice.
> 
7c9
< #           2014-2015  Vijayaditya Peddinti
---
> #           2014  Vijayaditya Peddinti
10,12d11
< # Terminology:
< # sample - one input-output tuple, which is an input sequence and output sequence for LSTM
< # frame  - one output label and the input context used to compute it
17c16
< num_epochs=10      # Number of epochs of training;
---
> num_epochs=15      # Number of epochs of training;
19,21c18,29
< initial_effective_lrate=0.0003
< final_effective_lrate=0.00003
< num_jobs_initial=1 # Number of neural net jobs to run in parallel at the start of training
---
> initial_effective_lrate=0.01
> final_effective_lrate=0.001
> pnorm_input_dim=3000
> pnorm_output_dim=300
> relu_dim=  # you can use this to make it use ReLU's instead of p-norms.
> rand_prune=4.0 # Relates to a speedup we do for LDA.
> minibatch_size=512  # This default is suitable for GPU-based training.
>                     # Set it to 128 for multi-threaded CPU-based training.
> max_param_change=2.0  # max param change per minibatch
> samples_per_iter=400000 # each iteration of training, see this many samples
>                         # per job.  This option is passed to get_egs.sh
> num_jobs_initial=1  # Number of neural net jobs to run in parallel at the start of training
27c35,36
< presoftmax_prior_scale_power=-0.25  # we haven't yet used pre-softmax prior scaling in the LSTM model
---
> presoftmax_prior_scale_power=-0.25
> use_presoftmax_prior_scale=true
48c57
< splice_indexes="-2,-1,0,1,2 0 0"
---
> splice_indexes="-4,-3,-2,-1,0,1,2,3,4  0  -2,2  0  -4,4 0"
53,95c62
< # LSTM parameters
< num_lstm_layers=3
< cell_dim=1024  # dimension of the LSTM cell
< hidden_dim=1024  # the dimension of the fully connected hidden layer outputs
< recurrent_projection_dim=256
< non_recurrent_projection_dim=256
< norm_based_clipping=true  # if true norm_based_clipping is used.
<                           # In norm-based clipping the activation Jacobian matrix
<                           # for the recurrent connections in the network is clipped
<                           # to ensure that the individual row-norm (l2) does not increase
<                           # beyond the clipping_threshold.
<                           # If false, element-wise clipping is used.
< clipping_threshold=30     # if norm_based_clipping is true this would be the maximum value of the row l2-norm,
<                           # else this is the max-absolute value of each element in Jacobian.
< chunk_width=20  # number of output labels in the sequence used to train an LSTM
<                 # Caution: if you double this you should halve --samples-per-iter.
< chunk_left_context=40  # number of steps used in the estimation of LSTM state before prediction of the first label
< chunk_right_context=0  # number of steps used in the estimation of LSTM state before prediction of the first label (usually used in bi-directional LSTM case)
< label_delay=5  # the lstm output is used to predict the label with the specified delay
< lstm_delay=" -1 -2 -3 "  # the delay to be used in the recurrence of lstms
<                          # "-1 -2 -3" means the a three layer stacked LSTM would use recurrence connections with
<                          # delays -1, -2 and -3 at layer1 lstm, layer2 lstm and layer3 lstm respectively
< 			 # "[-1,1] [-2,2] [-3,3]" means a three layer stacked bi-directional LSTM would use recurrence
< 			 # connections with delay -1 for the forward, 1 for the backward at layer1,
< 			 # -2 for the forward, 2 for the backward at layer2, and so on at layer3
< num_bptt_steps=    # this variable counts the number of time steps to back-propagate from the last label in the chunk
<                    # it is usually same as chunk_width
< 
< 
< # nnet3-train options
< shrink=0.99  # this parameter would be used to scale the parameter matrices
< shrink_threshold=0.15  # a value less than 0.25 that we compare the mean of
<                        # 'deriv-avg' for sigmoid components with, and if it's
<                        # less, we shrink.
< max_param_change=2.0  # max param change per minibatch
< num_chunk_per_minibatch=100  # number of sequences to be processed in parallel every mini-batch
< 
< samples_per_iter=20000 # this is really the number of egs in each archive.  Each eg has
<                        # 'chunk_width' frames in it-- for chunk_width=20, this value (20k)
<                        # is equivalent to the 400k number that we use as a default in
<                        # regular DNN training.
< momentum=0.5    # e.g. 0.5.  Note: we implemented it in such a way that
<                 # it doesn't increase the effective learning rate.
---
> randprune=4.0 # speeds up LDA.
102d68
< transform_dir=     # If supplied, this dir used instead of alidir to find transforms.
113,115d78
< 
< rand_prune=4.0 # speeds up LDA.
< 
116a80,81
> frames_per_eg=8 # to be passed on to get_egs.sh
> spk_num= # how many speakers + 1 silence
132,134c97,99
<   echo "  --num-epochs <#epochs|10>                        # Number of epochs of training"
<   echo "  --initial-effective-lrate <lrate|0.0003>         # effective learning rate at start of training."
<   echo "  --final-effective-lrate <lrate|0.00003>          # effective learning rate at end of training."
---
>   echo "  --num-epochs <#epochs|15>                        # Number of epochs of training"
>   echo "  --initial-effective-lrate <lrate|0.02> # effective learning rate at start of training."
>   echo "  --final-effective-lrate <lrate|0.004>   # effective learning rate at end of training."
136,138c101,104
<   echo "  --momentum <momentum|0.5>                        # Momentum constant: note, this is "
<   echo "                                                   # implemented in such a way that it doesn't"
<   echo "                                                   # increase the effective learning rate."
---
>   echo "  --num-hidden-layers <#hidden-layers|2>           # Number of hidden layers, e.g. 2 for 3 hours of data, 4 for 100hrs"
>   echo "  --add-layers-period <#iters|2>                   # Number of iterations between adding hidden layers"
>   echo "  --presoftmax-prior-scale-power <power|-0.25>     # use the specified power value on the priors (inverse priors) to scale"
>   echo "                                                   # the pre-softmax outputs (set to 0.0 to disable the presoftmax element scale)"
147c113,117
<   echo "  --splice-indexes <string|\"-2,-1,0,1,2 0 0\"> "
---
>   echo "  --minibatch-size <minibatch-size|128>            # Size of minibatch to process (note: product with --num-threads"
>   echo "                                                   # should not get too large, e.g. >2k)."
>   echo "  --samples-per-iter <#samples|400000>             # Number of samples of data to process per iteration, per"
>   echo "                                                   # process."
>   echo "  --splice-indexes <string|layer0/-4:-3:-2:-1:0:1:2:3:4> "
149,151c119
<   echo "                                                   # Format : <frame_indices> .... <frame_indices> "
<   echo "                                                   # the number of fields determines the number of LSTM and non-recurrent layers"
<   echo "                                                   # also see the --num-lstm-layers option"
---
>   echo "                                                   # Format : layer<hidden_layer_index>/<frame_indices>....layer<hidden_layer>/<frame_indices> "
154,155c122,123
<   echo "  --realign-epochs <list-of-epochs|''>             # A list of space-separated epoch indices the beginning of which"
<   echo "                                                   # realignment is to be done"
---
>   echo "  --realign-times <list-of-times|\"\">             # A list of space-separated floating point numbers between 0.0 and"
>   echo "                                                   # 1.0 to specify how far through training realignment is to be done"
162,194c130
<   echo " ################### LSTM options ###################### "
<   echo "  --num-lstm-layers <int|3>                        # number of LSTM layers"
<   echo "  --cell-dim   <int|1024>                          # dimension of the LSTM cell"
<   echo "  --hidden-dim      <int|1024>                     # the dimension of the fully connected hidden layer outputs"
<   echo "  --recurrent-projection-dim  <int|256>            # the output dimension of the recurrent-projection-matrix"
<   echo "  --non-recurrent-projection-dim  <int|256>        # the output dimension of the non-recurrent-projection-matrix"
<   echo "  --chunk-left-context <int|40>                    # number of time-steps used in the estimation of the first LSTM state"
<   echo "  --chunk-width <int|20>                           # number of output labels in the sequence used to train an LSTM"
<   echo "                                                   # Caution: if you double this you should halve --samples-per-iter."
<   echo "  --norm-based-clipping <bool|true>                # if true norm_based_clipping is used."
<   echo "                                                   # In norm-based clipping the activation Jacobian matrix"
<   echo "                                                   # for the recurrent connections in the network is clipped"
<   echo "                                                   # to ensure that the individual row-norm (l2) does not increase"
<   echo "                                                   # beyond the clipping_threshold."
<   echo "                                                   # If false, element-wise clipping is used."
<   echo "  --num-bptt-steps <int|>                          # this variable counts the number of time steps to back-propagate from the last label in the chunk"
<   echo "                                                   # it defaults to chunk_width"
<   echo "  --label-delay <int|5>                            # the lstm output is used to predict the label with the specified delay"
< 
<   echo "  --lstm-delay <str|\" -1 -2 -3 \">                # the delay to be used in the recurrence of lstms"
<   echo "                                                   # \"-1 -2 -3\" means the a three layer stacked LSTM would use recurrence connections with "
<   echo "                                                   # delays -1, -2 and -3 at layer1 lstm, layer2 lstm and layer3 lstm respectively"
<   echo "  --clipping-threshold <int|30>                    # if norm_based_clipping is true this would be the maximum value of the row l2-norm,"
<   echo "                                                   # else this is the max-absolute value of each element in Jacobian."
< 
<   echo " ################### LSTM specific training options ###################### "
<   echo "  --num-chunks-per-minibatch <minibatch-size|100>  # Number of sequences to be processed in parallel in a minibatch"
<   echo "  --samples-per-iter <#samples|20000>              # Number of egs in each archive of data.  This times --chunk-width is"
<   echo "                                                   # the number of frames processed per iteration"
<   echo "  --shrink <shrink|0.99>                           # if non-zero this parameter will be used to scale the parameter matrices"
<   echo "  --shrink-threshold <threshold|0.15>              # a threshold (should be between 0.0 and 0.25) that controls when to"
<   echo "                                                   # do parameter shrinking."
<   echo " for more options see the script"
---
> 
208d143
< done
212d146
< echo $nj > $dir/num_jobs
213a148
> echo $nj > $dir/num_jobs
216,217c151,152
< # model_left_context=(something)
< # model_right_context=(something)
---
> # left_context=(something)
> # right_context=(something)
220,221c155
< left_context=$((chunk_left_context + model_left_context))
< right_context=$((chunk_right_context + model_right_context))
---
> 
244,245c178,183
< chunk_width=$(cat $egs_dir/info/frames_per_eg) || { echo "error: no such file $egs_dir/info/frames_per_eg"; exit 1; }
< num_archives=$(cat $egs_dir/info/num_archives) || { echo "error: no such file $egs_dir/info/num_archives"; exit 1; }
---
> frames_per_eg=$(cat $egs_dir/info/frames_per_eg) || { echo "error: no such file $egs_dir/info/frames_per_eg"; exit 1; }
> num_archives=$(cat $egs_dir/info/num_archives) || { echo "error: no such file $egs_dir/info/frames_per_eg"; exit 1; }
> 
> # num_archives_expanded considers each separate label-position from
> # 0..frames_per_eg-1 to be a separate archive.
> num_archives_expanded=$[$num_archives*$frames_per_eg]
250,251c188,189
< [ $num_jobs_final -gt $num_archives ] && \
<   echo "$0: --final-num-jobs cannot exceed #archives $num_archives." && exit 1;
---
> [ $num_jobs_final -gt $num_archives_expanded ] && \
>   echo "$0: --final-num-jobs cannot exceed #archives $num_archives_expanded." && exit 1;
271c209
< # times, i.e. $num_iters*$avg_num_jobs) == $num_epochs*$num_archives,
---
> # times, i.e. $num_iters*$avg_num_jobs) == $num_epochs*$num_archives_expanded,
274c212
< num_archives_to_process=$[$num_epochs*$num_archives]
---
> num_archives_to_process=$[$num_epochs*$num_archives_expanded]
308c246,247
< approx_iters_per_epoch_final=$[$num_archives/$num_jobs_final]
---
> 
> approx_iters_per_epoch_final=$[$num_archives_expanded/$num_jobs_final]
337,338c276
< [ -z $num_bptt_steps ] && num_bptt_steps=$chunk_width;
< min_deriv_time=$((chunk_width - num_bptt_steps))
---
> 
345,346c283,285
<   this_effective_learning_rate=$(perl -e "print ($x + 1 >= $num_iters ? $flr : $ilr*exp($np*log($flr/$ilr)/$nt));");
<   this_learning_rate=$(perl -e "print ($this_effective_learning_rate*$this_num_jobs);");
---
>   this_learning_rate=$(perl -e "print (($x + 1 >= $num_iters ? $flr : $ilr*exp($np*log($flr/$ilr)/$nt))*$this_num_jobs);");
> 
>   echo "On iteration $x, learning rate is $this_learning_rate."
354,362d292
<     # Set this_shrink value.
<     if [ $x -eq 0 ] || nnet3-info --print-args=false $dir/$x.raw | \
<       perl -e "while(<>){ if (m/type=Sigmoid.+deriv-avg=.+mean=(\S+)/) { \$n++; \$tot+=\$1; } } exit(\$tot/\$n > $shrink_threshold);"; then
<       this_shrink=$shrink; # e.g. avg-deriv of sigmoids was <= 0.125, so shrink.
<     else
<       this_shrink=1.0  # don't shrink: sigmoids are not over-saturated.
<     fi
<     echo "On iteration $x, learning rate is $this_learning_rate and shrink value is $this_shrink."
< 
374d303
<         nnet3-info $dir/$x.raw '&&' \
376c305,306
<         "ark:nnet3-merge-egs --minibatch-size=256 ark:$cur_egs_dir/train_diagnostic.egs ark:-|" &
---
>         "ark:nnet3-merge-egs ark:$cur_egs_dir/train_diagnostic.egs ark:-|" '&&' \
>         nnet3-info $dir/$x.raw &
383c313
<     raw="nnet3-copy --learning-rate=$this_learning_rate $dir/$x.raw -|"
---
>     raw="nnet3-copy --learning-rate=$this_learning_rate $dir/$x.raw - |"
385c315
<       this_num_chunk_per_minibatch=$num_chunk_per_minibatch
---
>       this_minibatch_size=$minibatch_size
392c322
<       this_num_chunk_per_minibatch=$[$num_chunk_per_minibatch/2];
---
>       this_minibatch_size=$[$minibatch_size/2];
402c332
<       # We cannot easily use a single parallel SGE job to do the main training,
---
>       # We can't easily use a single parallel SGE job to do the main training,
405,407d334
<       # this is no longer true for RNNs as we use do not use the --frame option
<       # but we use the same script for consistency with FF-DNN code
< 
409c336
<         k=$[$num_archives_processed + $n - 1]; # k is a zero-based index that we will derive
---
>         k=$[$num_archives_processed + $n - 1]; # k is a zero-based index that we'll derive
411a339,343
>         frame=$[(($k/$num_archives)%$frames_per_eg)]; # work out the 0-based frame
>         # index; this increases more slowly than the archive index because the
>         # same archive with different frame indexes will give similar gradients,
>         # so we want to separate them in time.
> 
413,416c345,347
<           nnet3-train $parallel_train_opts --print-interval=10 --momentum=$momentum \
<           --max-param-change=$max_param_change \
<           --optimization.min-deriv-time=$min_deriv_time "$raw" \
<           "ark:nnet3-copy-egs $context_opts ark:$cur_egs_dir/egs.$archive.ark ark:- | nnet3-shuffle-egs --buffer-size=$shuffle_buffer_size --srand=$x ark:- ark:-| nnet3-merge-egs --minibatch-size=$this_num_chunk_per_minibatch --measure-output-frames=false --discard-partial-minibatches=true ark:- ark:- |" \
---
>           nnet3-train $parallel_train_opts \
>           --max-param-change=$max_param_change "$raw" \
>           "ark:nnet3-copy-egs --frame=$frame $context_opts ark:$cur_egs_dir/egs.$archive.ark ark:- | nnet3-shuffle-egs --buffer-size=$shuffle_buffer_size --srand=$x ark:- ark:-| nnet3-merge-egs --minibatch-size=$this_minibatch_size --discard-partial-minibatches=true ark:- ark:- |" \
425d355
<     models_to_average=$(steps/nnet3/get_successful_models.py $this_num_jobs $dir/log/train.$x.%.log)
427c357
<     for n in $models_to_average; do
---
>     for n in `seq 1 $this_num_jobs`; do
434,435c364
<         nnet3-average $nnets_list - \| \
<         nnet3-copy --scale=$this_shrink - $dir/$[$x+1].raw || exit 1;
---
>         nnet3-average $nnets_list $dir/$[$x+1].raw || exit 1;
442c371
<           $best_n=$n; } } print "$best_n\n"; ' $this_num_jobs $dir/log/train.$x.%d.log) || exit 1;
---
>           $best_n=$n; } } print "$best_n\n"; ' $num_jobs_nnet $dir/log/train.$x.%d.log) || exit 1;
445c374
<         nnet3-copy --scale=$this_shrink $dir/$[$x+1].$n.raw $dir/$[$x+1].raw || exit 1;
---
>         nnet3-copy $dir/$[$x+1].$n.raw $dir/$[$x+1].raw || exit 1;
448,452d376
<     nnets_list=
<     for n in `seq 1 $this_num_jobs`; do
<       nnets_list="$nnets_list $dir/$[$x+1].$n.raw"
<     done
< 
483c407
---
> 
487c411
---
>        --verbose=3 "${nnets_list[@]}" "ark:nnet3-merge-egs --minibatch-size=1024 ark:$cur_egs_dir/combine.egs ark:-|" \
495c419
<     "ark:nnet3-merge-egs --minibatch-size=256 ark:$cur_egs_dir/valid_diagnostic.egs ark:- |" &
---
>     "ark:nnet3-merge-egs ark:$cur_egs_dir/valid_diagnostic.egs ark:- |" &
498c422
<     "ark:nnet3-merge-egs --minibatch-size=256 ark:$cur_egs_dir/train_diagnostic.egs ark:- |" &
---
>     "ark:nnet3-merge-egs ark:$cur_egs_dir/train_diagnostic.egs ark:- |" &
526c450
<        rm $dir/$x.raw
---
>       rm $dir/$x.raw
